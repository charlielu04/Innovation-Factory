{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrsaDfdVHzxt"
      },
      "source": [
        "# Custom Training with YOLOv5\n",
        "\n",
        "In this tutorial, we assemble a dataset and train a custom YOLOv5 model to recognize the objects in our dataset. To do so we will take the following steps:\n",
        "\n",
        "* Gather a dataset of images and label our dataset\n",
        "* Export our dataset to YOLOv5\n",
        "* Train YOLOv5 to recognize the objects in our dataset\n",
        "* Evaluate our YOLOv5 model's performance\n",
        "* Run test inference to view our model at work\n",
        "\n",
        "\n",
        "\n",
        "![](https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNveqeA1KXGy"
      },
      "source": [
        "# Step 1: Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTvDNSILZoN9",
        "outputId": "bba69fdb-d2ca-4dc4-d5b9-847c176f7612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\charl\\VSCode Projects\\CMSC330\\project0\\project-1b-charlielu04\\Innovation-Factory\\yolo2\\yolov5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: Too many arguments.\n",
            "\n",
            "usage: git clone [<options>] [--] <repo> [<dir>]\n",
            "\n",
            "    -v, --verbose         be more verbose\n",
            "    -q, --quiet           be more quiet\n",
            "    --progress            force progress reporting\n",
            "    --reject-shallow      don't clone shallow repository\n",
            "    -n, --no-checkout     don't create a checkout\n",
            "    --bare                create a bare repository\n",
            "    --mirror              create a mirror repository (implies bare)\n",
            "    -l, --local           to clone from a local repository\n",
            "    --no-hardlinks        don't use local hardlinks, always copy\n",
            "    -s, --shared          setup as shared repository\n",
            "    --recurse-submodules[=<pathspec>]\n",
            "                          initialize submodules in the clone\n",
            "    --recursive ...       alias of --recurse-submodules\n",
            "    -j, --jobs <n>        number of submodules cloned in parallel\n",
            "    --template <template-directory>\n",
            "                          directory from which templates will be used\n",
            "    --reference <repo>    reference repository\n",
            "    --reference-if-able <repo>\n",
            "                          reference repository\n",
            "    --dissociate          use --reference only while cloning\n",
            "    -o, --origin <name>   use <name> instead of 'origin' to track upstream\n",
            "    -b, --branch <branch>\n",
            "                          checkout <branch> instead of the remote's HEAD\n",
            "    -u, --upload-pack <path>\n",
            "                          path to git-upload-pack on the remote\n",
            "    --depth <depth>       create a shallow clone of that depth\n",
            "    --shallow-since <time>\n",
            "                          create a shallow clone since a specific time\n",
            "    --shallow-exclude <revision>\n",
            "                          deepen history of shallow clone, excluding rev\n",
            "    --single-branch       clone only one branch, HEAD or --branch\n",
            "    --no-tags             don't clone any tags, and make later fetches not to follow them\n",
            "    --shallow-submodules  any cloned submodules will be shallow\n",
            "    --separate-git-dir <gitdir>\n",
            "                          separate git dir from working tree\n",
            "    -c, --config <key=value>\n",
            "                          set config inside the new repository\n",
            "    --server-option <server-specific>\n",
            "                          option to transmit\n",
            "    -4, --ipv4            use IPv4 addresses only\n",
            "    -6, --ipv6            use IPv6 addresses only\n",
            "    --filter <args>       object filtering\n",
            "    --also-filter-submodules\n",
            "                          apply partial clone filters to submodules\n",
            "    --remote-submodules   any cloned submodules will use their remote-tracking branch\n",
            "    --sparse              initialize sparse-checkout file to include only files at root\n",
            "    --bundle-uri <uri>    a URI for downloading bundles before fetching from origin remote\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Setup complete. Using torch 1.13.1+cpu (CPU)\n"
          ]
        }
      ],
      "source": [
        "#clone YOLOv5 and \n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install -q roboflow\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: roboflow in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (0.2.31)\n",
            "Requirement already satisfied: six in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (9.3.0)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (1.26.14)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (4.64.1)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (0.21.1)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (3.6.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (4.7.0.72)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (6.0)\n",
            "Requirement already satisfied: chardet==4.0.0 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: certifi==2022.12.7 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (2022.12.7)\n",
            "Requirement already satisfied: requests-toolbelt in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (0.10.1)\n",
            "Requirement already satisfied: cycler==0.10.0 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: requests in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (2.28.1)\n",
            "Requirement already satisfied: wget in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (3.2)\n",
            "Requirement already satisfied: idna==2.10 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from roboflow) (1.23.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.6)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from matplotlib->roboflow) (4.25.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from matplotlib->roboflow) (22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from matplotlib->roboflow) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\charl\\anaconda3\\envs\\innovationfactory\\lib\\site-packages (from requests->roboflow) (2.0.4)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in waldo-1 to yolov5pytorch: 100% [1818917 / 1818917] bytes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Dataset Version Zip to waldo-1 in yolov5pytorch:: 100%|██████████| 74/74 [00:00<00:00, 3359.66it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"4yGW1ceJlCEqjNUxNEuf\")\n",
        "project = rf.workspace(\"gaurigodghase-gmail-com\").project(\"waldo-cxaa8\")\n",
        "dataset = project.version(1).download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP6USLgz2f0r"
      },
      "source": [
        "# Step 2: Assemble Our Dataset\n",
        "\n",
        "In order to train our custom model, we need to assemble a dataset of representative images with bounding box annotations around the objects that we want to detect. And we need our dataset to be in YOLOv5 format.\n",
        "\n",
        "In Roboflow, you can choose between two paths:\n",
        "\n",
        "* Convert an existing dataset to YOLOv5 format. Roboflow supports over [30 formats object detection formats](https://roboflow.com/formats) for conversion.\n",
        "* Upload raw images and annotate them in Roboflow with [Roboflow Annotate](https://docs.roboflow.com/annotate).\n",
        "\n",
        "# Annotate\n",
        "\n",
        "![](https://roboflow-darknet.s3.us-east-2.amazonaws.com/roboflow-annotate.gif)\n",
        "\n",
        "# Version\n",
        "\n",
        "![](https://roboflow-darknet.s3.us-east-2.amazonaws.com/robolfow-preprocessing.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2wGvjd4Z_92",
        "outputId": "9b100d09-654e-4c01-e77d-71d3ec4264c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "upload and label your dataset, and get an API KEY here: https://app.roboflow.com/?model=yolov5&ref=ultralytics\n"
          ]
        }
      ],
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2jjT5uIHo6l5"
      },
      "outputs": [],
      "source": [
        "# set up environment\n",
        "os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwJcaoPGF4VI",
        "outputId": "4e22e5ac-6980-403e-a723-ba205596761c"
      },
      "outputs": [],
      "source": [
        "#after following the link above, recieve python code with these fields filled in\n",
        "#from roboflow import Roboflow\n",
        "#rf = Roboflow(api_key=\"YOUR API KEY HERE\")\n",
        "#project = rf.workspace().project(\"YOUR PROJECT\")\n",
        "#dataset = project.version(\"YOUR VERSION\").download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7yAi9hd-T4B"
      },
      "source": [
        "# Step 3: Train Our Custom YOLOv5 model\n",
        "\n",
        "Here, we are able to pass a number of arguments:\n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
        "- **data:** Our dataset locaiton is saved in the `dataset.location`\n",
        "- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n",
        "- **cache:** cache images for faster training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaFNnxLJbq4J",
        "outputId": "cd10b508-268b-4091-e76e-8aa3acf2984b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: train.py [-h] [--weights WEIGHTS] [--cfg CFG] [--data DATA] [--hyp HYP]\n",
            "                [--epochs EPOCHS] [--batch-size BATCH_SIZE] [--imgsz IMGSZ]\n",
            "                [--rect] [--resume [RESUME]] [--nosave] [--noval]\n",
            "                [--noautoanchor] [--noplots] [--evolve [EVOLVE]]\n",
            "                [--bucket BUCKET] [--cache [CACHE]] [--image-weights]\n",
            "                [--device DEVICE] [--multi-scale] [--single-cls]\n",
            "                [--optimizer {SGD,Adam,AdamW}] [--sync-bn] [--workers WORKERS]\n",
            "                [--project PROJECT] [--name NAME] [--exist-ok] [--quad]\n",
            "                [--cos-lr] [--label-smoothing LABEL_SMOOTHING]\n",
            "                [--patience PATIENCE] [--freeze FREEZE [FREEZE ...]]\n",
            "                [--save-period SAVE_PERIOD] [--seed SEED]\n",
            "                [--local_rank LOCAL_RANK] [--entity ENTITY]\n",
            "                [--upload_dataset [UPLOAD_DATASET]]\n",
            "                [--bbox_interval BBOX_INTERVAL]\n",
            "                [--artifact_alias ARTIFACT_ALIAS]\n",
            "train.py: error: unrecognized arguments: Projects\\CMSC330\\project0\\project-1b-charlielu04\\Innovation-Factory\\yolo2\\yolov5\\waldo-1/data.yaml\n"
          ]
        }
      ],
      "source": [
        "!python train.py --img 416 --batch 16 --epochs 150 --data {dataset.location}/data.yaml --weights yolov5s.pt --cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcIRLQOlA14A"
      },
      "source": [
        "# Evaluate Custom YOLOv5 Detector Performance\n",
        "Training losses and performance metrics are saved to Tensorboard and also to a logfile.\n",
        "\n",
        "If you are new to these metrics, the one you want to focus on is `mAP_0.5` - learn more about mean average precision [here](https://blog.roboflow.com/mean-average-precision/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1jS9_BxdBBHL"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-daf48b07a723ac62\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-daf48b07a723ac62\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "# logs save in the folder \"runs\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtmS7_TXFsT3"
      },
      "source": [
        "#Run Inference  With Trained Weights\n",
        "Run inference with a pretrained checkpoint on contents of `test/images` folder downloaded from Roboflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWjjiBcic3Vz",
        "outputId": "fc66484e-02b7-4e81-af74-6703dfea7c6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: detect.py [-h] [--weights WEIGHTS [WEIGHTS ...]] [--source SOURCE]\n",
            "                 [--data DATA] [--imgsz IMGSZ [IMGSZ ...]]\n",
            "                 [--conf-thres CONF_THRES] [--iou-thres IOU_THRES]\n",
            "                 [--max-det MAX_DET] [--device DEVICE] [--view-img]\n",
            "                 [--save-txt] [--save-conf] [--save-crop] [--nosave]\n",
            "                 [--classes CLASSES [CLASSES ...]] [--agnostic-nms]\n",
            "                 [--augment] [--visualize] [--update] [--project PROJECT]\n",
            "                 [--name NAME] [--exist-ok] [--line-thickness LINE_THICKNESS]\n",
            "                 [--hide-labels] [--hide-conf] [--half] [--dnn]\n",
            "                 [--vid-stride VID_STRIDE]\n",
            "detect.py: error: unrecognized arguments: Projects\\CMSC330\\project0\\project-1b-charlielu04\\Innovation-Factory\\yolo2\\yolov5\\waldo-1/test/images\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights runs/train/exp/weights/best.pt --img 416 --conf 0.1 --source {dataset.location}/test/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZbUn4_b9GCKO",
        "outputId": "70f601b7-d53d-4efd-fb97-6ae373b15b9f"
      },
      "outputs": [],
      "source": [
        "#display inference on ALL test images\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in glob.glob('/content/yolov5/runs/detect/exp/*.jpg'): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8dHcni6CJYt"
      },
      "source": [
        "# Conclusion and Next Steps\n",
        "\n",
        "Congratulations! You've trained a custom YOLOv5 model to recognize your custom objects.\n",
        "\n",
        "To improve you model's performance, we recommend first interating on your datasets coverage and quality. See this guide for [model performance improvement](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results).\n",
        "\n",
        "To deploy your model to an application, see this guide on [exporting your model to deployment destinations](https://github.com/ultralytics/yolov5/issues/251).\n",
        "\n",
        "Once your model is in production, you will want to continually iterate and improve on your dataset and model via [active learning](https://blog.roboflow.com/what-is-active-learning/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7iiObB2WCMh6",
        "outputId": "5c2c1e8a-a857-4284-d627-42f99724c9e8"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#export your model's weights for future use\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m files\n\u001b[0;32m      3\u001b[0m files\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39m./runs/train/exp/weights/best.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "#export your model's weights for future use\n",
        "from google.colab import files\n",
        "files.download('./runs/train/exp/weights/best.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNn-obvOGITm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "YOLOv5-Custom-Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "innovationFactory",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "01602fcf64e1cf82a0fb5d07e924522630da0482c7b734bbdc2e0b4c94891447"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
